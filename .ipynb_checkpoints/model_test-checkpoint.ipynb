{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df94fed-7277-41f2-9c59-47af6c811e8f",
   "metadata": {},
   "source": [
    "#  实验结果记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "576e473e-4a14-4188-9abd-04ff2a9e5dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 记录使用input anderson normalize(0, 1)后的结果\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951b22f3-275f-4d40-b502-70d7ba0090cc",
   "metadata": {},
   "source": [
    "# h5py\n",
    "```\n",
    "{\n",
    "\"model_000\":{\n",
    "    'train_loss_per_batch':[],\n",
    "    'train_loss_per_epoch': [],\n",
    "    'validate_loss_per_batch': [],  #可选\n",
    "    'validate_loss_per_epoch':[],    #可选\n",
    "    'test_loss_per_epoch':[],\n",
    "    'test_loss_per_batch':[]\n",
    "        \n",
    "    }\n",
    "\"model_001\":{}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385388f5-1a16-4c9b-91b0-9ee505527c31",
   "metadata": {},
   "source": [
    "# ok , the same\n",
    "compare old data the same with new formate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da12d803-38bb-4371-a3b2-a78f9374a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_epoch(file, title, n_model='model_000', train_only=False, begin=0):\n",
    "        with h5py.File(file, 'r') as loss_f:\n",
    "            train_grp = loss_f[n_model]\n",
    "            train_loss = train_grp['train_loss_per_epoch'][:]\n",
    "            validate_loss = train_grp['validate_loss_per_epoch'][:]\n",
    "\n",
    "            test_loss = train_grp['test_loss_per_epoch'][:]\n",
    "            # plot mse rather rmse , make confuse\n",
    "            train_loss = np.array(train_loss)\n",
    "            validate_loss = np.array(validate_loss)\n",
    "            test_loss = np.array(test_loss)\n",
    "            # plt.figure(figsize=(15, 5))\n",
    "            plt.plot(np.arange(len(train_loss) - begin)+1 + begin, train_loss[begin:], '-o', label='train')\n",
    "            if train_only is False:\n",
    "                plt.plot(np.arange(len(validate_loss) - begin)+1 + begin, validate_loss[begin:], '-o', label='validate')\n",
    "            plt.plot(len(train_loss), test_loss[0], '-o', label='test')\n",
    "            \n",
    "            plt.grid()\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('MSE')\n",
    "            plt.yscale('log')\n",
    "            plt.title(label=title)\n",
    "            plt.legend()\n",
    "            print(f'train_loss mse: {train_loss}')\n",
    "            print(f'validate_loss mse: {validate_loss}')\n",
    "            print(f'test_loss mse: {test_loss}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28211bbb-da76-404a-b25c-7ce548e520a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_loss(file_1, file_2, n_model='model_000', title='none'):\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12, 5))\n",
    "    fig.text(0.5, 0, 'Epoch')\n",
    "    fig.text(0, 0.5, 'MSE', rotation='vertical')\n",
    "    h5_1 = h5py.File(file_1, 'r')\n",
    "    train_grp_1 = h5_1[n_model]\n",
    "    train_loss_1 = train_grp_1['train_loss_per_epoch'][:]\n",
    "    validate_loss_1 = train_grp_1['validate_loss_per_epoch'][:]\n",
    "    test_loss_1 = train_grp_1['test_loss_per_epoch'][:]\n",
    "    print(f'file_1 train loss, mse: {train_loss_1[-5:]}')\n",
    "    print(f'file_1 test loss, mse: {test_loss_1}')\n",
    "\n",
    "    h5_2 = h5py.File(file_2, 'r')\n",
    "    train_grp_2 = h5_2[n_model]\n",
    "    train_loss_2 = train_grp_2['train_loss_per_epoch'][:]\n",
    "    validate_loss_2 = train_grp_2['validate_loss_per_epoch'][:]\n",
    "    print(f'validate {validate_loss_2}')\n",
    "    test_loss_2 = train_grp_2['test_loss_per_epoch'][:]\n",
    "    print(f'file_2 train loss, mse: {train_loss_2[-5:]}')\n",
    "    print(f'file_2 test loss, mse: {test_loss_2}')\n",
    "    print()\n",
    "    \n",
    "    axes[0].plot(train_loss_1, '-o', label=f'{file_1}')\n",
    "    axes[0].plot(train_loss_2, '-o', label=f'{file_2}')\n",
    "    axes[0].set_title('train')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid()\n",
    "    axes[0].set_yscale('log')\n",
    "    \n",
    "    axes[1].plot(validate_loss_1, '-o', label=f'{file_1}')\n",
    "    axes[1].plot(validate_loss_2, '-o', label=f'{file_2}')\n",
    "    axes[1].set_title('validate')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid()\n",
    "    axes[1].set_yscale('log')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc2c40a-7935-4633-bef5-1ff8d84cfcfc",
   "metadata": {},
   "source": [
    "## layer 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f558f555-9629-49c2-bdcf-9533fc2605a7",
   "metadata": {},
   "source": [
    "## test1\n",
    "使用 chebyshev_median.h5中的中位数，4000的训练集，查看模型的loss。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1690d9-c198-4f90-8d45-76b8d94ee96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = \"datasets/chebyshev_0_4000.h5\"\n",
    "testing_file = \"datasets/chebyshev_testing_1000.h5\",\n",
    "# train data\n",
    "from run_mlp import main\n",
    "# main('config_1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f78685-ba92-4884-9876-28ec8badd7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot epoch loss\n",
    "plot_loss_epoch(file='nn_7/loss_1.h5', title=\"median norm 4000 nn7\", begin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af6f820-a3d2-456d-bbb2-2093351c805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'nn_7/loss_1.h5'\n",
    "h5 = h5py.File(file, 'r')\n",
    "h5['model_000']['log_dict']['train_loss_per_batch'][-8:],h5['model_000']['log_dict']['validate_loss_per_batch'][-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cc83d7-7b29-4868-80d8-fc5bdd448247",
   "metadata": {},
   "source": [
    "# layer 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0200d834-2d07-462b-a257-fd910eebf794",
   "metadata": {},
   "source": [
    "## test2\n",
    "与test1 相比，使用MyMLP_14，查看模型的loss。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adc8a3c-44c3-454e-ac6b-0a66516487b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'nn_14/loss_2.h5'\n",
    "h5 = h5py.File(file, 'r')\n",
    "h5['model_000']['log_dict']['train_loss_per_batch'][-8:],h5['model_000']['log_dict']['validate_loss_per_batch'][-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3694f9-9459-4019-8570-124d5c7f9fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_epoch(file='nn_14/loss_2.h5', title=\"median norm 4000 nn14\", begin=0)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc26172-3c7a-4a10-81e2-0c08e6fa94ef",
   "metadata": {},
   "source": [
    "从上面看， 配置不变，网络加深，loss是变小了。,说明7层，模型过于简单"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "675c84e3-5db8-4c41-9a1f-c38bccb2ae76",
   "metadata": {},
   "source": [
    "## test3\n",
    "与test2相比，训练集变为10000，查看模型的loss。\n",
    "- Increase the size of your training dataset\n",
    "\n",
    "在比较数据集大小时，其他保持不变，训练数据从4000->10000, loss变小一些。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e70129-b7fd-49b8-8480-497441f98607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use L6N255_10000.h5 to train nn-7\n",
    "training_file = \"datasets/chebyshev_0_10000.h5\"\n",
    "testing_file = \"datasets/chebyshev_testing_1000.h5\"\n",
    "config = 'config_3.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981c142b-c4cd-48ef-8a5f-ed6d5575e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "del loss_f\n",
    "loss_f = 'nn_14/loss_3.h5'\n",
    "plot_loss_epoch(loss_f, title=\"median norm 10000 nn14\")\n",
    "# 看着效果没有改善"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee6c2df-c4b6-424e-9113-f50b2265d379",
   "metadata": {},
   "outputs": [],
   "source": [
    "del file_1, file_2\n",
    "file_1 = 'nn_14/loss_2.h5'\n",
    "file_2 = 'nn_14/loss_3.h5'\n",
    "compare_loss(file_1, file_2)\n",
    "# 图 左边为 train size:4000, 右边为train size:10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfddb36-6598-4a8f-8a97-d59876875c95",
   "metadata": {},
   "source": [
    "## test4\n",
    "增加batch_size:1024----> 2048\n",
    "差别不大，但是batch_size选择不一样，会导致loss 抖动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d80bf8-0d39-4936-a26f-6caaf26fa53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del file_1, file_2\n",
    "file_1 = 'nn_14/loss_3.h5'   # 1024\n",
    "file_2 = 'nn_14/loss_4.h5'   # 2048\n",
    "compare_loss(file_1, file_2, title='batchsize 1024 ---> 2048')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6d6c20-67e6-46c7-841e-5337419d2d53",
   "metadata": {},
   "source": [
    "## test5\n",
    "与test4相比，数据集改成将anderson模型规范化，规范化为8个维度，每个维度的均值方差记在10000_norm_meta.h5，Chebyshev系数与test4一样，也是使用median规范化的。\n",
    "- Normalize your input data\n",
    "Scaling the input features to a similar range can prevent certain features from dominating the learning process and help the MLP model converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f3a170-894e-4c69-bab3-9139358cc1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "del file_1, file_2\n",
    "file_1 = 'nn_14/loss_4.h5'   # test4\n",
    "file_2 = \"nn_14/loss_5.h5\"   # test5\n",
    "compare_loss(file_1, file_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ecfc3f-2639-4cad-a48a-6e9868e77016",
   "metadata": {},
   "source": [
    "## **Adjust the architecture of your MLP**: \n",
    "Experiment with different numbers of hidden layers, neurons per layer, and activation functions to find the optimal configuration for your specific problem.\n",
    "> resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f0cbd7-9918-49cc-8765-16cc5b0488e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
